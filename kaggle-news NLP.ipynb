{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "version": "3.6.4",
      "file_extension": ".py",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "name": "python",
      "mimetype": "text/x-python"
    },
    "colab": {
      "name": "HW2_Deep_Learner.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eg6fVa_SOmSM"
      },
      "source": [
        "**HW#2 - YCBS 273 Intro to Prac ML**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uGAIl2vuORna"
      },
      "source": [
        "# Coding report"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jsQVNT7_AanA"
      },
      "source": [
        "## Importing data and libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G4o_Z9SfHNR5"
      },
      "source": [
        "# import the necessary libraries\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import zipfile\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "import tensorflow_hub as hub\n",
        "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ConWyEyOI9o"
      },
      "source": [
        "import zipfile\n",
        "with zipfile.ZipFile('data_v2.zip', 'r') as zip_ref: \n",
        "    zip_ref.extractall('data')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2VxRrGeXBjUK"
      },
      "source": [
        "## Doing text vectorization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ki300bSeH1go",
        "outputId": "5f0164f8-5fd5-4852-ad4f-56fe83214ffc"
      },
      "source": [
        "batch_size = 512\n",
        "seed = 1337 # Keep the seed same for both 'train' & 'validation' to avoid overlap\n",
        "\n",
        "train_ds = keras.preprocessing.text_dataset_from_directory(\n",
        "    'data/train', \n",
        "    batch_size=batch_size,\n",
        "    label_mode='int',\n",
        "    validation_split=0.1,\n",
        "    subset='training',\n",
        "    seed=seed)\n",
        "\n",
        "val_ds = keras.preprocessing.text_dataset_from_directory(\n",
        "    'data/train',\n",
        "    batch_size=batch_size,\n",
        "    label_mode='int',\n",
        "    validation_split=0.1,\n",
        "    subset='validation',\n",
        "    seed=seed)\n",
        "\n",
        "text_only_train_ds = train_ds.map(lambda x, y: x) # It is used to train the vectorization"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 120000 files belonging to 4 classes.\n",
            "Using 114000 files for training.\n",
            "Found 120000 files belonging to 4 classes.\n",
            "Using 6000 files for validation.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-08-11T16:27:16.010174Z",
          "iopub.execute_input": "2021-08-11T16:27:16.010811Z",
          "iopub.status.idle": "2021-08-11T16:27:16.050429Z",
          "shell.execute_reply.started": "2021-08-11T16:27:16.010777Z",
          "shell.execute_reply": "2021-08-11T16:27:16.049199Z"
        },
        "trusted": true,
        "id": "lZFbZag9IAWa"
      },
      "source": [
        "max_length = 100\n",
        "max_tokens = 20000\n",
        "text_vectorization = TextVectorization(\n",
        "    max_tokens=max_tokens,\n",
        "    output_mode=\"int\",\n",
        "    output_sequence_length=max_length,\n",
        ")\n",
        "\n",
        "text_vectorization.adapt(text_only_train_ds)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-08-11T16:27:16.815822Z",
          "iopub.execute_input": "2021-08-11T16:27:16.816185Z",
          "iopub.status.idle": "2021-08-11T16:27:17.002676Z",
          "shell.execute_reply.started": "2021-08-11T16:27:16.816154Z",
          "shell.execute_reply": "2021-08-11T16:27:17.001438Z"
        },
        "trusted": true,
        "id": "QLlpC75mIAWb"
      },
      "source": [
        "seq_train_ds = train_ds.map(lambda x, y: (text_vectorization(x), y))\n",
        "seq_val_ds = val_ds.map(lambda x, y: (text_vectorization(x), y))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SAswGoxlCP7d"
      },
      "source": [
        "## Loading the GloVe embeddings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UnjFdrghiMHD"
      },
      "source": [
        "!unzip -q glove.6B.100d.txt.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NbAiejRSiQ_q",
        "outputId": "ec93b372-3e2b-4a1a-c9fa-80e34a1225b5"
      },
      "source": [
        "path_to_glove_file = \"glove.6B.100d.txt\"\n",
        "\n",
        "embeddings_index = {}\n",
        "with open(path_to_glove_file) as f:\n",
        "    for line in f:\n",
        "        word, coefs = line.split(maxsplit=1)\n",
        "        coefs = np.fromstring(coefs, \"f\", sep=\" \")\n",
        "        embeddings_index[word] = coefs\n",
        "\n",
        "print(f\"Found {len(embeddings_index)} word vectors.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 400000 word vectors.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E6xqKbMPksqC"
      },
      "source": [
        "embedding_dim = 100\n",
        "\n",
        "vocabulary = text_vectorization.get_vocabulary()\n",
        "word_index = dict(zip(vocabulary, range(len(vocabulary))))\n",
        "\n",
        "embedding_matrix = np.zeros((max_tokens, embedding_dim))\n",
        "for word, i in word_index.items():\n",
        "    if i < max_tokens:\n",
        "        embedding_vector = embeddings_index.get(word)\n",
        "    if embedding_vector is not None:\n",
        "        embedding_matrix[i] = embedding_vector"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1PH3E8vqlF15"
      },
      "source": [
        "embedding_layer = layers.Embedding(\n",
        "    max_tokens,\n",
        "    embedding_dim,\n",
        "    embeddings_initializer=keras.initializers.Constant(embedding_matrix),\n",
        "    trainable=False,\n",
        "    mask_zero=True,\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wybj4KWxPG0T"
      },
      "source": [
        "## Building a sequential model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "std_berlPXa5"
      },
      "source": [
        "### RNN model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-08-11T16:27:18.079876Z",
          "iopub.execute_input": "2021-08-11T16:27:18.080269Z",
          "iopub.status.idle": "2021-08-11T16:56:53.289021Z",
          "shell.execute_reply.started": "2021-08-11T16:27:18.080238Z",
          "shell.execute_reply": "2021-08-11T16:56:53.287931Z"
        },
        "trusted": true,
        "id": "ZzYrS47dIAWb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d2c67f6a-aa81-4e0c-83f1-c383366f897e"
      },
      "source": [
        "# RNN\n",
        "inputs = keras.Input(shape=(None,), dtype=\"int64\")\n",
        "embedded = embedding_layer(inputs)\n",
        "x = layers.Bidirectional(layers.LSTM(32))(embedded)\n",
        "x = layers.Dropout(0.5)(x)\n",
        "x = layers.Dense(64, activation=\"relu\")(x)\n",
        "x = layers.Dropout(0.5)(x)\n",
        "x = layers.Dense(32, activation=\"relu\")(x)\n",
        "x = layers.Dropout(0.5)(x)\n",
        "outputs = layers.Dense(4, activation=\"softmax\")(x)\n",
        "model = keras.Model(inputs, outputs)\n",
        "\n",
        "model.compile(optimizer=\"rmsprop\",\n",
        "       loss=\"sparse_categorical_crossentropy\",\n",
        "       metrics=[\"accuracy\"])\n",
        "\n",
        "model.summary()\n",
        "\n",
        "callbacks = [\n",
        "       keras.callbacks.EarlyStopping(monitor=\"val_loss\",\n",
        "                       patience=2),\n",
        "             \n",
        "       keras.callbacks.ModelCheckpoint(\"sequential.keras\",\n",
        "                       monitor=\"val_loss\",\n",
        "                       save_best_only=True)\n",
        "]\n",
        "\n",
        "model.fit(seq_train_ds.cache(),\n",
        "          validation_data = seq_val_ds,\n",
        "          epochs=10,\n",
        "          callbacks=callbacks,\n",
        "          )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_2 (InputLayer)         [(None, None)]            0         \n",
            "_________________________________________________________________\n",
            "embedding (Embedding)        (None, None, 100)         2000000   \n",
            "_________________________________________________________________\n",
            "bidirectional_1 (Bidirection (None, 64)                34048     \n",
            "_________________________________________________________________\n",
            "dropout_3 (Dropout)          (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 64)                4160      \n",
            "_________________________________________________________________\n",
            "dropout_4 (Dropout)          (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 32)                2080      \n",
            "_________________________________________________________________\n",
            "dropout_5 (Dropout)          (None, 32)                0         \n",
            "_________________________________________________________________\n",
            "dense_5 (Dense)              (None, 4)                 132       \n",
            "=================================================================\n",
            "Total params: 2,040,420\n",
            "Trainable params: 40,420\n",
            "Non-trainable params: 2,000,000\n",
            "_________________________________________________________________\n",
            "Epoch 1/10\n",
            "188/188 [==============================] - 52s 232ms/step - loss: 0.7056 - accuracy: 0.7416 - val_loss: 0.3759 - val_accuracy: 0.8808\n",
            "Epoch 2/10\n",
            "188/188 [==============================] - 8s 44ms/step - loss: 0.4515 - accuracy: 0.8673 - val_loss: 0.3515 - val_accuracy: 0.8854\n",
            "Epoch 3/10\n",
            "188/188 [==============================] - 9s 47ms/step - loss: 0.4072 - accuracy: 0.8799 - val_loss: 0.3390 - val_accuracy: 0.8910\n",
            "Epoch 4/10\n",
            "188/188 [==============================] - 8s 44ms/step - loss: 0.3832 - accuracy: 0.8856 - val_loss: 0.3195 - val_accuracy: 0.8938\n",
            "Epoch 5/10\n",
            "188/188 [==============================] - 9s 49ms/step - loss: 0.3688 - accuracy: 0.8892 - val_loss: 0.3182 - val_accuracy: 0.8965\n",
            "Epoch 6/10\n",
            "188/188 [==============================] - 8s 43ms/step - loss: 0.3567 - accuracy: 0.8917 - val_loss: 0.3149 - val_accuracy: 0.8931\n",
            "Epoch 7/10\n",
            "188/188 [==============================] - 10s 53ms/step - loss: 0.3440 - accuracy: 0.8956 - val_loss: 0.3090 - val_accuracy: 0.8960\n",
            "Epoch 8/10\n",
            "188/188 [==============================] - 9s 47ms/step - loss: 0.3365 - accuracy: 0.8970 - val_loss: 0.3055 - val_accuracy: 0.9019\n",
            "Epoch 9/10\n",
            "188/188 [==============================] - 9s 50ms/step - loss: 0.3280 - accuracy: 0.8990 - val_loss: 0.2987 - val_accuracy: 0.9042\n",
            "Epoch 10/10\n",
            "188/188 [==============================] - 10s 52ms/step - loss: 0.3194 - accuracy: 0.9021 - val_loss: 0.2980 - val_accuracy: 0.9053\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fd387342f90>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AoVnD24qJKV3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dfd3c009-1ba4-40af-d183-c9cea74514f1"
      },
      "source": [
        "# RNN\n",
        "model2 = keras.models.load_model(\"sequential.keras\")\n",
        "\n",
        "prediction_model = tf.keras.Sequential(\n",
        "    [text_vectorization, model2])\n",
        "\n",
        "prediction_model.compile(\n",
        "    loss=keras.losses.SparseCategoricalCrossentropy(),\n",
        "    optimizer='adam',\n",
        "    metrics=['accuracy'])\n",
        "\n",
        "loss, accuracy = prediction_model.evaluate(val_ds)\n",
        "print(\"Accuracy: {:2.2%}\".format(accuracy))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "47/47 [==============================] - 15s 143ms/step - loss: 0.2980 - accuracy: 0.9053\n",
            "Accuracy: 90.53%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HbqKNXxWL-3W"
      },
      "source": [
        "### Transformer model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WX-yLADV62bP"
      },
      "source": [
        "# define a class\n",
        "class TransformerEncoder(layers.Layer):\n",
        "    def __init__(self, embed_dim, dense_dim, num_heads, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.embed_dim = embed_dim\n",
        "        self.dense_dim = dense_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.attention = layers.MultiHeadAttention(\n",
        "            num_heads=num_heads, key_dim=embed_dim)\n",
        "        self.dense_proj = keras.Sequential(\n",
        "            [layers.Dense(dense_dim, activation=\"relu\"),\n",
        "             layers.Dense(embed_dim),]\n",
        "        )\n",
        "        self.layernorm_1 = layers.LayerNormalization()\n",
        "        self.layernorm_2 = layers.LayerNormalization()\n",
        "\n",
        "    def call(self, inputs, mask=None):\n",
        "        if mask is not None:\n",
        "            mask = mask[:, tf.newaxis, :]\n",
        "        attention_output = self.attention(\n",
        "            inputs, inputs, attention_mask=mask)\n",
        "        proj_input = self.layernorm_1(inputs + attention_output)\n",
        "        proj_output = self.dense_proj(proj_input)\n",
        "        return self.layernorm_2(proj_input + proj_output)\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super().get_config()\n",
        "        config.update({\n",
        "            \"embed_dim\": self.embed_dim,\n",
        "            \"num_heads\": self.num_heads,\n",
        "            \"dense_dim\": self.dense_dim,\n",
        "        })\n",
        "        return config"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-08-11T16:27:18.079876Z",
          "iopub.execute_input": "2021-08-11T16:27:18.080269Z",
          "iopub.status.idle": "2021-08-11T16:56:53.289021Z",
          "shell.execute_reply.started": "2021-08-11T16:27:18.080238Z",
          "shell.execute_reply": "2021-08-11T16:56:53.287931Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LODj4LlM7Ocq",
        "outputId": "85f586b7-a415-4252-fb00-72a38ba027a9"
      },
      "source": [
        "# transformer\n",
        "embed_dim = 100\n",
        "num_heads = 2\n",
        "dense_dim = 32\n",
        "inputs = keras.Input(shape=(None,), dtype=\"int64\")\n",
        "embedded = embedding_layer(inputs)\n",
        "x = TransformerEncoder(embed_dim, dense_dim, num_heads)(embedded)\n",
        "x = layers.GlobalMaxPooling1D()(x)\n",
        "x = layers.Dropout(0.5)(x)\n",
        "\n",
        "outputs = layers.Dense(4, activation=\"softmax\")(x)\n",
        "model = keras.Model(inputs, outputs)\n",
        "\n",
        "model.compile(optimizer=\"rmsprop\",\n",
        "       loss=\"sparse_categorical_crossentropy\",\n",
        "       metrics=[\"accuracy\"])\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_5 (InputLayer)         [(None, None)]            0         \n",
            "_________________________________________________________________\n",
            "embedding_1 (Embedding)      (None, None, 100)         2000000   \n",
            "_________________________________________________________________\n",
            "transformer_encoder_3 (Trans (None, None, 100)         87632     \n",
            "_________________________________________________________________\n",
            "global_max_pooling1d_4 (Glob (None, 100)               0         \n",
            "_________________________________________________________________\n",
            "dropout_3 (Dropout)          (None, 100)               0         \n",
            "_________________________________________________________________\n",
            "dense_12 (Dense)             (None, 4)                 404       \n",
            "=================================================================\n",
            "Total params: 2,088,036\n",
            "Trainable params: 88,036\n",
            "Non-trainable params: 2,000,000\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_ZXw_NZj8PQP",
        "outputId": "0e6646c9-4522-442c-c2b7-263c7e158ce4"
      },
      "source": [
        "# transformer\n",
        "callbacks = [\n",
        "       keras.callbacks.EarlyStopping(monitor=\"val_loss\",\n",
        "                       patience=2),\n",
        "             \n",
        "       keras.callbacks.ModelCheckpoint(\"transformer.keras\",\n",
        "                       monitor=\"val_loss\",\n",
        "                       save_best_only=True)\n",
        "]\n",
        "\n",
        "model.fit(seq_train_ds.cache(),\n",
        "     validation_data = seq_val_ds,\n",
        "     epochs=25,\n",
        "     callbacks=callbacks,\n",
        "    )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/25\n",
            "223/223 [==============================] - 87s 382ms/step - loss: 0.5687 - accuracy: 0.8122 - val_loss: 0.3134 - val_accuracy: 0.8923\n",
            "Epoch 2/25\n",
            "223/223 [==============================] - 10s 46ms/step - loss: 0.3301 - accuracy: 0.8895 - val_loss: 0.3008 - val_accuracy: 0.8968\n",
            "Epoch 3/25\n",
            "223/223 [==============================] - 17s 74ms/step - loss: 0.3133 - accuracy: 0.8947 - val_loss: 0.2907 - val_accuracy: 0.9002\n",
            "Epoch 4/25\n",
            "223/223 [==============================] - 11s 49ms/step - loss: 0.3023 - accuracy: 0.8979 - val_loss: 0.2894 - val_accuracy: 0.9012\n",
            "Epoch 5/25\n",
            "223/223 [==============================] - 11s 48ms/step - loss: 0.2940 - accuracy: 0.9003 - val_loss: 0.2828 - val_accuracy: 0.9037\n",
            "Epoch 6/25\n",
            "223/223 [==============================] - 11s 48ms/step - loss: 0.2875 - accuracy: 0.9024 - val_loss: 0.2779 - val_accuracy: 0.9065\n",
            "Epoch 7/25\n",
            "223/223 [==============================] - 10s 47ms/step - loss: 0.2823 - accuracy: 0.9030 - val_loss: 0.2726 - val_accuracy: 0.9078\n",
            "Epoch 8/25\n",
            "223/223 [==============================] - 11s 48ms/step - loss: 0.2760 - accuracy: 0.9057 - val_loss: 0.2738 - val_accuracy: 0.9068\n",
            "Epoch 9/25\n",
            "223/223 [==============================] - 11s 48ms/step - loss: 0.2717 - accuracy: 0.9072 - val_loss: 0.2703 - val_accuracy: 0.9083\n",
            "Epoch 10/25\n",
            "223/223 [==============================] - 10s 46ms/step - loss: 0.2674 - accuracy: 0.9086 - val_loss: 0.2648 - val_accuracy: 0.9102\n",
            "Epoch 11/25\n",
            "223/223 [==============================] - 11s 47ms/step - loss: 0.2628 - accuracy: 0.9097 - val_loss: 0.2635 - val_accuracy: 0.9097\n",
            "Epoch 12/25\n",
            "223/223 [==============================] - 10s 47ms/step - loss: 0.2582 - accuracy: 0.9114 - val_loss: 0.2631 - val_accuracy: 0.9095\n",
            "Epoch 13/25\n",
            "223/223 [==============================] - 11s 47ms/step - loss: 0.2546 - accuracy: 0.9127 - val_loss: 0.2632 - val_accuracy: 0.9102\n",
            "Epoch 14/25\n",
            "223/223 [==============================] - 10s 47ms/step - loss: 0.2509 - accuracy: 0.9136 - val_loss: 0.2640 - val_accuracy: 0.9093\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7faf477f0610>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TZMYzw6Q_tiX",
        "outputId": "19e97e19-6c54-43a6-f55d-a0341bce9080"
      },
      "source": [
        "# transformer\n",
        "# Using the trained model to make prediction on unseen (test) data\n",
        "# Here we use the 'adapted' text_vectorization layer and include it as part of a prediction_model\n",
        "\n",
        "model2 = keras.models.load_model(\n",
        "    \"transformer.keras\",\n",
        "    custom_objects={\"TransformerEncoder\": TransformerEncoder})\n",
        "\n",
        "prediction_model = tf.keras.Sequential(\n",
        "    [text_vectorization, model2])\n",
        "\n",
        "prediction_model.compile(\n",
        "    loss=keras.losses.SparseCategoricalCrossentropy(),\n",
        "    optimizer='adam',\n",
        "    metrics=['accuracy'])\n",
        "\n",
        "# Test it with `val_ds`, which yields raw strings\n",
        "loss, accuracy = prediction_model.evaluate(val_ds)\n",
        "print(\"Accuracy: {:2.2%}\".format(accuracy))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "12/12 [==============================] - 2s 51ms/step - loss: 0.2631 - accuracy: 0.9095\n",
            "Accuracy: 90.95%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tgh_xd7nNkAG"
      },
      "source": [
        "## Recording the results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HW8axK9SImmY"
      },
      "source": [
        "df_test_data = pd.read_csv('data/data_test_df.csv')\n",
        "inputs = df_test_data['data']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WN9gA8SjJPyn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dea8245f-2f62-4f12-a212-8590590addff"
      },
      "source": [
        "# Make sure you use the 'prediction_model' and not the trained 'model' alone\n",
        "# If you use the 'model' object, you will run int error as the data is still in the 'text' format and needs vectorization\n",
        "\n",
        "predicted_scores = prediction_model.predict(inputs)\n",
        "predicted_scores[0:5]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[5.8026876e-02, 3.4643817e-04, 9.3418193e-01, 7.4447407e-03],\n",
              "       [1.2452911e-02, 4.2287694e-04, 5.8042642e-04, 9.8654383e-01],\n",
              "       [6.0894764e-03, 7.3628509e-03, 6.8372384e-02, 9.1817528e-01],\n",
              "       [1.1799880e-02, 7.2705969e-02, 1.2470035e-01, 7.9079384e-01],\n",
              "       [4.1254425e-01, 1.8902628e-03, 4.2133275e-02, 5.4343218e-01]],\n",
              "      dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "AgjDvZ6XIAWc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 390
        },
        "outputId": "05fbb093-c636-4109-a486-03c464cda28b"
      },
      "source": [
        "# populating the dataframe to make a submission on Kaggle\n",
        "\n",
        "df_predictions = pd.DataFrame(predicted_scores, columns=['solution_' + str(i+1) for i in range(4)])\n",
        "df_predictions.index.rename('Id', inplace=True)\n",
        "\n",
        "df_predictions.head(10)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>solution_1</th>\n",
              "      <th>solution_2</th>\n",
              "      <th>solution_3</th>\n",
              "      <th>solution_4</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Id</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.058027</td>\n",
              "      <td>0.000346</td>\n",
              "      <td>0.934182</td>\n",
              "      <td>0.007445</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.012453</td>\n",
              "      <td>0.000423</td>\n",
              "      <td>0.000580</td>\n",
              "      <td>0.986544</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.006089</td>\n",
              "      <td>0.007363</td>\n",
              "      <td>0.068372</td>\n",
              "      <td>0.918175</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.011800</td>\n",
              "      <td>0.072706</td>\n",
              "      <td>0.124700</td>\n",
              "      <td>0.790794</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.412544</td>\n",
              "      <td>0.001890</td>\n",
              "      <td>0.042133</td>\n",
              "      <td>0.543432</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>0.123073</td>\n",
              "      <td>0.000524</td>\n",
              "      <td>0.027899</td>\n",
              "      <td>0.848504</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>0.005756</td>\n",
              "      <td>0.000131</td>\n",
              "      <td>0.003011</td>\n",
              "      <td>0.991102</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>0.021702</td>\n",
              "      <td>0.001688</td>\n",
              "      <td>0.026682</td>\n",
              "      <td>0.949927</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>0.086772</td>\n",
              "      <td>0.000145</td>\n",
              "      <td>0.032083</td>\n",
              "      <td>0.881000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>0.044639</td>\n",
              "      <td>0.006712</td>\n",
              "      <td>0.045626</td>\n",
              "      <td>0.903023</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    solution_1  solution_2  solution_3  solution_4\n",
              "Id                                                \n",
              "0     0.058027    0.000346    0.934182    0.007445\n",
              "1     0.012453    0.000423    0.000580    0.986544\n",
              "2     0.006089    0.007363    0.068372    0.918175\n",
              "3     0.011800    0.072706    0.124700    0.790794\n",
              "4     0.412544    0.001890    0.042133    0.543432\n",
              "5     0.123073    0.000524    0.027899    0.848504\n",
              "6     0.005756    0.000131    0.003011    0.991102\n",
              "7     0.021702    0.001688    0.026682    0.949927\n",
              "8     0.086772    0.000145    0.032083    0.881000\n",
              "9     0.044639    0.006712    0.045626    0.903023"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MCMahjclIAWc"
      },
      "source": [
        "df_predictions.to_csv('df_predictions.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cCezAJURRcvy"
      },
      "source": [
        "## [failed] Attempt to do NLP data augmentation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OMhvgxEDCm-c",
        "outputId": "cd381b9f-d57d-4f70-b2b0-6328d9831bc3"
      },
      "source": [
        "!pip install nlpaug"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting nlpaug\n",
            "  Downloading nlpaug-1.1.7-py3-none-any.whl (405 kB)\n",
            "\u001b[?25l\r\u001b[K     |▉                               | 10 kB 25.8 MB/s eta 0:00:01\r\u001b[K     |█▋                              | 20 kB 28.6 MB/s eta 0:00:01\r\u001b[K     |██▍                             | 30 kB 17.4 MB/s eta 0:00:01\r\u001b[K     |███▎                            | 40 kB 15.2 MB/s eta 0:00:01\r\u001b[K     |████                            | 51 kB 10.8 MB/s eta 0:00:01\r\u001b[K     |████▉                           | 61 kB 12.2 MB/s eta 0:00:01\r\u001b[K     |█████▋                          | 71 kB 13.0 MB/s eta 0:00:01\r\u001b[K     |██████▌                         | 81 kB 14.3 MB/s eta 0:00:01\r\u001b[K     |███████▎                        | 92 kB 12.8 MB/s eta 0:00:01\r\u001b[K     |████████                        | 102 kB 11.0 MB/s eta 0:00:01\r\u001b[K     |█████████                       | 112 kB 11.0 MB/s eta 0:00:01\r\u001b[K     |█████████▊                      | 122 kB 11.0 MB/s eta 0:00:01\r\u001b[K     |██████████▌                     | 133 kB 11.0 MB/s eta 0:00:01\r\u001b[K     |███████████▎                    | 143 kB 11.0 MB/s eta 0:00:01\r\u001b[K     |████████████▏                   | 153 kB 11.0 MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 163 kB 11.0 MB/s eta 0:00:01\r\u001b[K     |█████████████▊                  | 174 kB 11.0 MB/s eta 0:00:01\r\u001b[K     |██████████████▋                 | 184 kB 11.0 MB/s eta 0:00:01\r\u001b[K     |███████████████▍                | 194 kB 11.0 MB/s eta 0:00:01\r\u001b[K     |████████████████▏               | 204 kB 11.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 215 kB 11.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████▉              | 225 kB 11.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████▋             | 235 kB 11.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████▍            | 245 kB 11.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████▎           | 256 kB 11.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 266 kB 11.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▉          | 276 kB 11.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▋         | 286 kB 11.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▌        | 296 kB 11.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▎       | 307 kB 11.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 317 kB 11.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▉      | 327 kB 11.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▊     | 337 kB 11.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▌    | 348 kB 11.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▎   | 358 kB 11.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▏  | 368 kB 11.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 378 kB 11.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▊ | 389 kB 11.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▌| 399 kB 11.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 405 kB 11.0 MB/s \n",
            "\u001b[?25hInstalling collected packages: nlpaug\n",
            "Successfully installed nlpaug-1.1.7\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "11h_hUSQCe5z"
      },
      "source": [
        "import nlpaug.augmenter.word as naw"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H2Z5R4SVDe5C",
        "outputId": "6c7c6b07-6cb7-4907-8edc-2329abc38f50"
      },
      "source": [
        "#swap\n",
        "text = 'I like machine learning.'\n",
        "aug = naw.RandomWordAug(action=\"swap\")\n",
        "augmented_text = aug.augment(text)\n",
        "print(\"Original:\")\n",
        "print(text)\n",
        "print(\"Augmented Text:\")\n",
        "print(augmented_text)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Original:\n",
            "I like machine learning.\n",
            "Augmented Text:\n",
            "I like learning machine.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ELAD4O1YE_OA",
        "outputId": "dfe1d568-fd2d-4447-bef8-7c120ae4a194"
      },
      "source": [
        "#delete\n",
        "aug = naw.RandomWordAug()\n",
        "augmented_text = aug.augment(text)\n",
        "print(\"Original:\")\n",
        "print(text)\n",
        "print(\"Augmented Text:\")\n",
        "print(augmented_text)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Original:\n",
            "I like machine learning.\n",
            "Augmented Text:\n",
            "Like machine learning.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SZKRvr_-YiqF"
      },
      "source": [
        "import os\n",
        "path = 'data/train'\n",
        "list_catalog=os.listdir(path) \n",
        "for root, dirs, files in os.walk(path):\n",
        "  for f in files:\n",
        "    with open(os.path.join(root,f),\"r\") as f:    \n",
        "      str = f.read()  \n",
        "      print(str)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oq8MKflmZ6dU",
        "outputId": "bbfdfdaa-93fe-42d2-866c-b3940fb608f6"
      },
      "source": [
        "list_catalog"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['3', '4', '1', '2']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 82
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KqKgIGznJMX-"
      },
      "source": [
        "count = 0\n",
        "for text_batch, label_batch in train_ds:\n",
        "  print(len(text_batch))\n",
        "  print(len(label_batch))\n",
        "  for i in range(2):\n",
        "    text = bytes.decode(text_batch.numpy()[i])\n",
        "    aug = naw.RandomWordAug()\n",
        "    augmented_text = aug.augment(text)\n",
        "    print(\"Original:\")\n",
        "    print(text)\n",
        "    print(\"Augmented Text:\")\n",
        "    print(augmented_text)\n",
        "    count = count+1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PpGj6B2pSwoQ"
      },
      "source": [
        "# Kaggle submission journal"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qclu5ZwaS-ND"
      },
      "source": [
        "## Submission 1&2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kHMuO1VDTBen"
      },
      "source": [
        "We reversed the test dataset and training dataset when we submitted it for the first time, kaggle rejected it since the number of rows was wrong.\n",
        "Submission 2 was the version one of benchmark code.<br>\n",
        "**Score: 0.56234**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x_2azoAmUdYe"
      },
      "source": [
        "## Submission 3-5"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E-qo2lTKUnhS"
      },
      "source": [
        "In submission 3, Xiaotong Xu added one more dense layer to the binary unigram model.<br>\n",
        "**Score: 0.56233**<br>\n",
        "In submission 4, Xiaotong Xu increased the max_tokens to 30000.<br>\n",
        "**Score: 0.56233**<br>\n",
        "In submission 5, Xiaotong Xu used binary bigram model with \"tf-idf\" text vectoriazion.<br>\n",
        "**Score: 0.56232**<br>\n",
        "From submission 3 to submission 4, we got to know that max_tokens of 20000 was enough for this dataset. The performance improved little under the \"tf-idf\" text vectorization, we discussed a lot but could not figure out the reason."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3vFqbNqmYYOi"
      },
      "source": [
        "## Submission 6"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bV72XjArYdaq"
      },
      "source": [
        "In the class, we learned that it was essential to do text vectorization on the test dataset, and teacher gave us an example to label the training dataset. Then we downloaded the newest data and tried the RNN model based on benchmark code v2.<br>\n",
        "**Score: 0.17201**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0LZrmONXaNRv"
      },
      "source": [
        "## Submission 7"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZDaFR1z6aa6o"
      },
      "source": [
        "We added one more dense layer to the RNN model and had 128 nodes in embedding layer. The result indicated that a complex layer was not good. Jiehao Wan said that this was because the data for embedding training were too small. Therefore, we turned to a pretrained embedding dataset in the next time.<br>\n",
        "**Score: 0.18624**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JDynSKS3bfSs"
      },
      "source": [
        "## Submission 8"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rwR83jx8biKw"
      },
      "source": [
        "In the breakout room exercise, we took an arduous journey to fix the bug of importing pretraied embedding layer. With the help of teachers, Jiehao Wan made a submission and the score improved a lot. <br>\n",
        "**Score: 0.13760**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a3884jQkcqEf"
      },
      "source": [
        "## Submission 9"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lscQZvkLcx7h"
      },
      "source": [
        "We read the relevant literature and found that in the field of natural language processing, the transformer model is the most popular. Since the RNN model had little to improve, we tried to adopt the transformer model. The score improved litte, which disappointed us. However, after the competition, we noticed that transformer model performed well in the private dataset.<br>\n",
        "**Score: 0.13753**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9gGZRxIDe6M2"
      },
      "source": [
        "## Submission 10"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LI1onlzQgEfe"
      },
      "source": [
        "Weihang Fu tried an advanced Bow model to make a comparison.<br>\n",
        "It seemed that transformer model was better.<br>\n",
        "**Score: 0.13991**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8RMbWoUlgzf-"
      },
      "source": [
        "## Submission 11"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3jBhPwPCg3xr"
      },
      "source": [
        "Xiaotong Xu used positional embeddings instead of GloVe embeddings.<br>\n",
        "We also attempted to use the bert model but failed.<br>\n",
        "**Score: 0.15036**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J1ySQzl_hf1c"
      },
      "source": [
        "## Submission 12"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PJdFdniYhimk"
      },
      "source": [
        "We no longer changed the methods of embedding. <br>\n",
        "In the next stage, we wanted to do some data augmentation. We knew that data augmentation is useful in image processing, but had no idea about the application on NLP. Then we read the paper 《EDA: Easy Data Augmentation Techniques for Boosting Performance on Text Classification Tasks》 and found out some methods to do it. At first, we doubted whether the changes on the sentences would distort the meanings. Xiaotong Xu said that it may be feasible,  since it is a classfication problem rather than a translation task. Unfortunately, we didn't know how to generate the new data in txt format. The code always told us a compile error.<br>\n",
        "In another way, we \"increased\" the training data by adjusting the ratio of the training dataset and the validation dataset to 9:1.<br>\n",
        "**Score: 0.13557**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AaX_OjU1ls4_"
      },
      "source": [
        "## Submission 13-15"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2iDynU3Hlu8c"
      },
      "source": [
        "In this stage, we focused on the model itself.<br>\n",
        "We wanted to obtain the best hyperparameters through Bayesian optimization.<br>\n",
        "In the end, we adopted dense_dim=64 and num_heads=2.<br>\n",
        "It seemed that score was improved.<br>\n",
        "**Score: 0.13339 (best)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d5RM4GaUnKM9"
      },
      "source": [
        "## Submission 16"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gUUa5hesnN84"
      },
      "source": [
        "In the process of randomly printing the sentences of the dataset, Xiaotong Xu found that the average length of the sentences exceeded 50, which means that setting max_length of 50 will lose some important information. Therefore we increased the max_length to 100.<br>\n",
        "**Score: 0.12732**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9YeGc6fwod-b"
      },
      "source": [
        "## Final score"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pl1MrCMAoqt2"
      },
      "source": [
        "**Public leaderboard: 0.12897 (ranked 4th)**<br>\n",
        "**Private leaderboard: 0.12732 (ranked 3rd)**<br> \n",
        "For both assignments, we worked hard from the bottom of the leaderboard and finally got a satisfactory model result. This was a very interesting and challenging process and we enjoyed the exploration of machine learning a lot."
      ]
    }
  ]
}